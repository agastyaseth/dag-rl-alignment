{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e41ca9-2808-4fe8-bed9-de687feee56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q torch torchvision diffusers opencv-python moviepy ffmpeg-python numpy pillow ipython\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "from pyramid_dit import PyramidDiTForVideoGeneration\n",
    "from diffusers.utils import export_to_video\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pyramid_dit.flux_modules.modeling_text_encoder import FluxTextEncoderWithMask\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML, display\n",
    "from pyramid_dit import PyramidDiTForVideoGeneration\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model path and configuration\n",
    "model_path = \"/pyramid-flow-miniflux\"  # Replace with your Pyramid Flow checkpoint directory\n",
    "model_dtype = \"bf16\"  # Options: \"bf16\", \"fp16\", \"fp32\"\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the Pyramid Flow model\n",
    "variant = \"diffusion_transformer_384p\"  # Use 384p variant for now\n",
    "model = PyramidDiTForVideoGeneration(\n",
    "    model_path,\n",
    "    model_dtype,\n",
    "    model_name=\"pyramid_flux\",\n",
    "    model_variant=variant,\n",
    ")\n",
    "\n",
    "# Move components to GPU\n",
    "model.vae.to(device)\n",
    "model.dit.to(device)\n",
    "model.text_encoder.to(device)\n",
    "model.vae.enable_tiling()\n",
    "\n",
    "# Set torch dtype\n",
    "if model_dtype == \"bf16\":\n",
    "    torch_dtype = torch.bfloat16\n",
    "elif model_dtype == \"fp16\":\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdeef9-e3c8-4a50-94e0-e5ff8fee79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTextDataset(Dataset):\n",
    "    def __init__(self, video_dir, metadata_path, resolution=384, num_frames=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_dir (str): Directory containing video files.\n",
    "            metadata_path (str): Path to the metadata CSV file with columns 'video_filename' and 'caption'.\n",
    "            resolution (int): Target resolution for video frames.\n",
    "            num_frames (int): Number of frames to sample from each video.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        self.resolution = resolution\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get video file path and corresponding caption\n",
    "        video_filename = self.metadata.iloc[idx][\"video_filename\"]\n",
    "        caption = self.metadata.iloc[idx][\"caption\"]\n",
    "        video_path = os.path.join(self.video_dir, video_filename)\n",
    "\n",
    "        # Process video\n",
    "        frames = self._process_video(video_path)\n",
    "        return frames, caption\n",
    "\n",
    "    def _process_video(self, video_path):\n",
    "        \"\"\"Load and process video frames.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = max(1, total_frames // self.num_frames)\n",
    "\n",
    "        for i in range(0, total_frames, step):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Convert frame to RGB and resize\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (self.resolution, self.resolution), interpolation=cv2.INTER_AREA)\n",
    "            frame = torch.tensor(frame).permute(2, 0, 1) / 255.0  # Normalize to [0, 1]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Pad frames if fewer than num_frames are available\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        return torch.stack(frames)  # Shape: [num_frames, 3, resolution, resolution]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5bd7d-4c64-4bf2-8de3-769972bfee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "video_dir = \"./finetune_dataset/video/files\"  # Replace with your video directory\n",
    "metadata_path = \"./finetune_dataset/metadata.csv\"  # Replace with your metadata CSV file\n",
    "\n",
    "# Dataset and DataLoader parameters\n",
    "resolution = 384  # Video resolution (aligned with the model variant)\n",
    "num_frames = 16  # Number of frames per video\n",
    "batch_size = 4  # Batch size for training\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = VideoTextDataset(video_dir, metadata_path, resolution=resolution, num_frames=num_frames)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Verify DataLoader output\n",
    "for frames, captions in dataloader:\n",
    "    print(\"Frames shape:\", frames.shape)  # [batch_size, num_frames, 3, resolution, resolution]\n",
    "    print(\"Captions:\", captions)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddbbad-147a-4b1d-933b-44df70a79116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FluxTextEncoder\n",
    "text_encoder = FluxTextEncoderWithMask(model_path, torch_dtype).to(device)\n",
    "\n",
    "# Define a function to encode prompts\n",
    "def encode_prompts(prompts, text_encoder, num_images_per_prompt=1, device=device):\n",
    "    \"\"\"\n",
    "    Encode textual prompts using the FluxTextEncoder.\n",
    "\n",
    "    Args:\n",
    "        prompts (list[str]): List of textual prompts.\n",
    "        text_encoder (FluxTextEncoderWithMask): Initialized text encoder model.\n",
    "        num_images_per_prompt (int): Number of video generations per prompt.\n",
    "        device (torch.device): Device for computation.\n",
    "\n",
    "    Returns:\n",
    "        prompt_embeds: T5 embeddings for the prompts.\n",
    "        prompt_attention_mask: Attention masks for the T5 embeddings.\n",
    "        pooled_prompt_embeds: CLIP embeddings for the prompts.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, prompt_attention_mask, pooled_prompt_embeds = text_encoder.encode_prompt(\n",
    "            prompts, num_images_per_prompt=num_images_per_prompt, device=device\n",
    "        )\n",
    "    return prompt_embeds, prompt_attention_mask, pooled_prompt_embeds\n",
    "\n",
    "# Test the text encoder with sample prompts\n",
    "test_prompts = [\"A cinematic video of a spaceship landing on Mars\", \"A dog running in the park\"]\n",
    "\n",
    "# Encode prompts\n",
    "prompt_embeds, prompt_attention_mask, pooled_prompt_embeds = encode_prompts(test_prompts, text_encoder)\n",
    "\n",
    "# Display the shapes of the embeddings\n",
    "print(\"T5 Prompt Embeddings Shape:\", prompt_embeds.shape)  # [batch_size, seq_len, hidden_dim]\n",
    "print(\"Prompt Attention Mask Shape:\", prompt_attention_mask.shape)  # [batch_size, seq_len]\n",
    "print(\"CLIP Pooled Embeddings Shape:\", pooled_prompt_embeds.shape)  # [batch_size, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73dba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Model\n",
    "# here, I use CLIP to extract the feature and text embeddings. They are then combined into one tensor. \n",
    "# the tensor is turned into a scalar value using a few sequential layers, which forms the reward score.\n",
    "class RAFTRewardModel(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-large-patch14\"):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, frames, prompts):\n",
    "        clip_features = self.clip.get_image_features(frames)\n",
    "        text_features = self.clip.get_text_features(prompts)\n",
    "        combined_features = torch.cat([clip_features, text_features], dim=-1)\n",
    "        return self.reward_head(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21651262-a8a0-496f-8866-886b67eb3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained CLIP model for evaluating the alignment between generated frames and the textual prompt\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "def evaluate_aesthetic_score(frames, text_prompt, clip_model, clip_processor, device):\n",
    "    \"\"\"\n",
    "    Evaluate the aesthetic alignment between video frames and a text prompt.\n",
    "\n",
    "    Args:\n",
    "        frames (torch.Tensor): A tensor of video frames with shape [num_frames, 3, height, width].\n",
    "        text_prompt (str): The textual description for the video.\n",
    "        clip_model (CLIPModel): Pre-trained CLIP model for scoring.\n",
    "        clip_processor (CLIPProcessor): CLIP processor for preparing inputs.\n",
    "        device (torch.device): The device for computation (e.g., \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        float: The aesthetic alignment score between the frames and the text prompt.\n",
    "    \"\"\"\n",
    "    # Prepare video frames for CLIP\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        frame_np = frame.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "        processed_frame = clip_processor(images=frame_np, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        processed_frames.append(processed_frame)\n",
    "\n",
    "    processed_frames = torch.cat(processed_frames).to(device)  # Shape: [num_frames, 3, height, width]\n",
    "\n",
    "    # Encode the text prompt\n",
    "    text_inputs = clip_processor(text=[text_prompt], return_tensors=\"pt\", padding=True).to(device)\n",
    "    text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "    # Encode the video frames\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(processed_frames)\n",
    "\n",
    "    # Normalize features for cosine similarity\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute aesthetic alignment score as the average cosine similarity\n",
    "    scores = torch.matmul(image_features, text_features.T).mean().item()\n",
    "    return scores\n",
    "\n",
    "# Example usage\n",
    "sample_frames = torch.rand((16, 3, 384, 384))  # Replace with actual video frames (16 frames, 384x384 resolution)\n",
    "sample_prompt = \"A cinematic video of a spaceship landing on Mars\"\n",
    "\n",
    "# Evaluate the score\n",
    "aesthetic_score = evaluate_aesthetic_score(sample_frames, sample_prompt, clip_model, clip_processor, device)\n",
    "print(\"Aesthetic Score:\", aesthetic_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948eba6-55bd-4f26-986c-191287cc77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for fine-tuning the text-to-video model\n",
    "\n",
    "# @markdown Enter value for `resolution`.\n",
    "resolution = 384  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown Enter value for `num_frames` (number of frames per video).\n",
    "num_frames = 16  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown Enter value for `batch_size`.\n",
    "batch_size = 4  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown Enter value for `num_inference_steps`.\n",
    "num_inference_steps = [20, 20, 20]  # @param {type:\"raw\"}  # Per pyramid level inference steps\n",
    "\n",
    "# @markdown Enter value for `guidance_scale`.\n",
    "guidance_scale = 7.0  # @param {type:\"number\"}  # Strength of text conditioning for initial frame\n",
    "\n",
    "# @markdown Enter value for `video_guidance_scale`.\n",
    "video_guidance_scale = 5.0  # @param {type:\"number\"}  # Strength of text conditioning for subsequent frames\n",
    "\n",
    "# @markdown Enter value for `learning_rate`.\n",
    "learning_rate = 1e-5  # @param {type:\"number\"}  # Learning rate for fine-tuning\n",
    "\n",
    "# @markdown Enter value for `epochs`.\n",
    "epochs = 10  # @param {type:\"integer\"}  # Number of training epochs\n",
    "\n",
    "# @markdown Enter value for `fps`.\n",
    "fps = 24  # @param {type:\"integer\"}  # Frames per second for generated videos\n",
    "\n",
    "# @markdown Enter value for `num_candidates`.\n",
    "num_candidates = 8 #8 candidates for each and every input prompt.\n",
    "\n",
    "# Display selected parameters\n",
    "print(f\"Resolution: {resolution}x{resolution}\")\n",
    "print(f\"Number of Frames per Video: {num_frames}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Number of Inference Steps per Pyramid Level: {num_inference_steps}\")\n",
    "print(f\"Guidance Scale (Initial Frame): {guidance_scale}\")\n",
    "print(f\"Video Guidance Scale (Subsequent Frames): {video_guidance_scale}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Number of Epochs: {epochs}\")\n",
    "print(f\"Frames per Second (FPS): {fps}\")\n",
    "print(f\"Number of candidates for RAFT fine-tuning: {num_candidates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654afd4-282a-4182-ae2e-68f2179e62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory for saving the model exists\n",
    "output_dir = \"./fine_tuned_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "video_dataset = VideoTextDataset(\n",
    "    video_dir=video_dir,\n",
    "    metadata_path=metadata_path,\n",
    "    resolution=resolution,\n",
    "    num_frames=num_frames\n",
    ")\n",
    "video_dataloader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define loss function\n",
    "reconstruction_criterion = nn.MSELoss()  # Reconstruction loss for frame quality\n",
    "aesthetic_weight = 0.5  # Weight for aesthetic score in the loss function\n",
    "\n",
    "pyramid_model = PyramidDiTForVideoGeneration(\n",
    "    model_path=model_path,\n",
    "    model_dtype=torch_dtype,\n",
    "    model_name=\"pyramid_flux\",\n",
    "    model_variant=variant,\n",
    ").to(device)\n",
    "\n",
    "# Load CLIP model for aesthetic evaluation\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "#modified iterative fine-tunign with RAFT principles\n",
    "#Three main principles to be followed: data generation(n candidates per prompt), reward ranking, finetuning\n",
    "def train_with_raft(model, reward_model, dataloader, num_candidates, epochs, \n",
    "                    learning_rate, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs * len(dataloader)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            frames, captions = batch\n",
    "            frames = frames.to(device)\n",
    "            \n",
    "            prompt_embeds, prompt_attention_mask, pooled_prompt_embeds = text_encoder.encode_prompt(\n",
    "            captions, num_images_per_prompt=1, device=device\n",
    "        )\n",
    "            # generate 8 candidates  \n",
    "            candidates = []\n",
    "            candidate_scores = []\n",
    "            \n",
    "            for _ in range(num_candidates):\n",
    "                with torch.no_grad():\n",
    "                    generated_frames = model.forward(\n",
    "                        prompt_embeds=prompt_embeds,\n",
    "                        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                        video_guidance_scale=video_guidance_scale,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        height=resolution,\n",
    "                        width=resolution,\n",
    "                        num_inference_steps=num_inference_steps\n",
    "                    )\n",
    "                    candidates.append(generated_frames)\n",
    "                    \n",
    "                    # reward socres \n",
    "                    score = reward_model(generated_frames, captions)\n",
    "                    candidate_scores.append(score)\n",
    "            \n",
    "            # get preference using softmax normalisation. the resulting distribution favors candidates with higher reward scores.\n",
    "            candidate_scores = torch.stack(candidate_scores)\n",
    "            preferences = F.softmax(candidate_scores, dim=0)\n",
    "            \n",
    "            # regenarate samples with non zero preference, and weight the final reconstruction loss with the above computed preferences.\n",
    "            raft_loss = 0\n",
    "            for i in range(num_candidates):\n",
    "                if preferences[i] > 0:\n",
    "                    generated = model.forward(\n",
    "                        prompt_embeds=prompt_embeds,\n",
    "                        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                        video_guidance_scale=video_guidance_scale,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        height=resolution,\n",
    "                        width=resolution,\n",
    "                        num_inference_steps=num_inference_steps\n",
    "                    )\n",
    "                    reconstruction_loss = reconstruction_criterion(generated, candidates[i])\n",
    "                    raft_loss += preferences[i] * reconstruction_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            raft_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += raft_loss.item()\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": running_loss / (progress_bar.n + 1),\n",
    "                \"best_score\": torch.max(candidate_scores).item()\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {running_loss / len(dataloader):.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        model_save_path = os.path.join(output_dir, f\"raft_model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': running_loss,\n",
    "        }, model_save_path)\n",
    "        print(f\"Model checkpoint saved at {model_save_path}\")\n",
    "\n",
    "    print(\"Fine-tuning complete!\")\n",
    "\n",
    "# init reward model\n",
    "reward_model = RAFTRewardModel().to(device)\n",
    "\n",
    "# train with raft \n",
    "train_with_raft(\n",
    "    model=pyramid_model,\n",
    "    reward_model=reward_model,\n",
    "    dataloader=video_dataloader,\n",
    "    num_candidates=num_candidates,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46149873-3ff4-44b7-aac9-6fdf55987ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display generated videos\n",
    "def show_video(video_path, width=\"70%\"):\n",
    "    \"\"\"\n",
    "    Displays a video in the notebook.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        width (str): Width of the video display.\n",
    "    \"\"\"\n",
    "    html = f\"\"\"\n",
    "    <video controls style=\"width: {width};\">\n",
    "        <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"\n",
    "    return HTML(html)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model_path = \"./finetune_model\"  # Replace with the actual path\n",
    "\n",
    "# Initialize the fine-tuned model\n",
    "pyramid_model = PyramidDiTForVideoGeneration(\n",
    "    model_path=fine_tuned_model_path,\n",
    "    model_dtype=torch_dtype,\n",
    "    model_name=\"pyramid_flux\",\n",
    "    model_variant=\"diffusion_transformer_384p\",\n",
    ").to(device)\n",
    "\n",
    "# Set generation parameters\n",
    "test_prompts = [\n",
    "    \"A cinematic video of a spaceship landing on Mars\",\n",
    "    \"A dog running in a beautiful park during sunset\"\n",
    "]\n",
    "num_frames = 16\n",
    "resolution = 384\n",
    "num_inference_steps = [20, 20, 20]\n",
    "guidance_scale = 7.0\n",
    "video_guidance_scale = 5.0\n",
    "fps = 24\n",
    "\n",
    "# Generate and display videos for each prompt\n",
    "for idx, prompt in enumerate(test_prompts):\n",
    "    print(f\"Generating video for prompt: '{prompt}'\")\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "        frames = pyramid_model.generate(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=resolution,\n",
    "            width=resolution,\n",
    "            temp=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            video_guidance_scale=video_guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "        )\n",
    "\n",
    "    # Export frames to a video file\n",
    "    video_path = f\"generated_video_{idx + 1}.mp4\"\n",
    "    export_to_video(frames, video_path, fps=fps)\n",
    "\n",
    "    # Display the video in the notebook\n",
    "    display(show_video(video_path))\n",
    "\n",
    "print(\"Video generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
