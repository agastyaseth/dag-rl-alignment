{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import blobfile as bf\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import seed_everything\n",
    "from tqdm import tqdm\n",
    "\n",
    "from arguments import parse_args\n",
    "from models import get_model, get_multi_apply_fn\n",
    "from rewards import get_reward_losses\n",
    "from training import LatentNoiseTrainer, get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    seed_everything(args.seed)\n",
    "    bf.makedirs(f\"{args.save_dir}/logs/{args.task}\")\n",
    "    # Set up logging and name settings\n",
    "    logger = logging.getLogger()\n",
    "    settings = (\n",
    "        f\"{args.model}{'_' + args.prompt if args.task == 't2i-compbench' else ''}\"\n",
    "        f\"{'_no-optim' if args.no_optim else ''}_{args.seed if args.task != 'geneval' else ''}\"\n",
    "        f\"_lr{args.lr}_gc{args.grad_clip}_iter{args.n_iters}\"\n",
    "        f\"_reg{args.reg_weight if args.enable_reg else '0'}\"\n",
    "        f\"{'_pickscore' + str(args.pickscore_weighting) if args.enable_pickscore else ''}\"\n",
    "        f\"{'_clip' + str(args.clip_weighting) if args.enable_clip else ''}\"\n",
    "        f\"{'_hps' + str(args.hps_weighting) if args.enable_hps else ''}\"\n",
    "        f\"{'_imagereward' + str(args.imagereward_weighting) if args.enable_imagereward else ''}\"\n",
    "        f\"{'_aesthetic' + str(args.aesthetic_weighting) if args.enable_aesthetic else ''}\"\n",
    "    )\n",
    "    file_stream = open(f\"{args.save_dir}/logs/{args.task}/{settings}.txt\", \"w\")\n",
    "    handler = logging.StreamHandler(file_stream)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(\"INFO\")\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(formatter)\n",
    "    logger.addHandler(consoleHandler)\n",
    "    logging.info(args)\n",
    "    if args.device_id is not None:\n",
    "        logging.info(f\"Using CUDA device {args.device_id}\")\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_id\n",
    "    device = torch.device(\"cuda\")\n",
    "    if args.dtype == \"float32\":\n",
    "        dtype = torch.float32\n",
    "    elif args.dtype == \"float16\":\n",
    "        dtype = torch.float16\n",
    "    # Get reward losses\n",
    "    reward_losses = get_reward_losses(args, dtype, device, args.cache_dir)\n",
    "\n",
    "    # Get model and noise trainer\n",
    "    pipe = get_model(\n",
    "        args.model, dtype, device, args.cache_dir, args.memsave, args.cpu_offloading\n",
    "    )\n",
    "    trainer = LatentNoiseTrainer(\n",
    "        reward_losses=reward_losses,\n",
    "        model=pipe,\n",
    "        n_iters=args.n_iters,\n",
    "        n_inference_steps=args.n_inference_steps,\n",
    "        seed=args.seed,\n",
    "        save_all_images=args.save_all_images,\n",
    "        device=device,\n",
    "        no_optim=args.no_optim,\n",
    "        regularize=args.enable_reg,\n",
    "        regularization_weight=args.reg_weight,\n",
    "        grad_clip=args.grad_clip,\n",
    "        log_metrics=args.task == \"single\" or not args.no_optim,\n",
    "        imageselect=args.imageselect,\n",
    "    )\n",
    "\n",
    "    # Create latents\n",
    "    if args.model == \"flux\":\n",
    "        # currently only support 512x512 generation\n",
    "        shape = (1, 16 * 64, 64)\n",
    "    elif args.model != \"pixart\":\n",
    "        height = pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
    "        width = pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
    "        shape = (\n",
    "            1,\n",
    "            pipe.unet.in_channels,\n",
    "            height // pipe.vae_scale_factor,\n",
    "            width // pipe.vae_scale_factor,\n",
    "        )\n",
    "    else:\n",
    "        height = pipe.transformer.config.sample_size * pipe.vae_scale_factor\n",
    "        width = pipe.transformer.config.sample_size * pipe.vae_scale_factor\n",
    "        shape = (\n",
    "            1,\n",
    "            pipe.transformer.config.in_channels,\n",
    "            height // pipe.vae_scale_factor,\n",
    "            width // pipe.vae_scale_factor,\n",
    "        )\n",
    "    enable_grad = not args.no_optim\n",
    "    if args.enable_multi_apply:\n",
    "        multi_apply_fn = get_multi_apply_fn(\n",
    "            model_type=args.multi_step_model,\n",
    "            seed=args.seed,\n",
    "            pipe=pipe,\n",
    "            cache_dir=args.cache_dir,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "    else:\n",
    "        multi_apply_fn = None\n",
    "\n",
    "    if args.task == \"single\":\n",
    "        init_latents = torch.randn(shape, device=device, dtype=dtype)\n",
    "        latents = torch.nn.Parameter(init_latents, requires_grad=enable_grad)\n",
    "        optimizer = get_optimizer(args.optim, latents, args.lr, args.nesterov)\n",
    "        save_dir = f\"{args.save_dir}/{args.task}/{settings}/{args.prompt[:150]}\"\n",
    "        os.makedirs(f\"{save_dir}\", exist_ok=True)\n",
    "        init_image, best_image, total_init_rewards, total_best_rewards = trainer.train(\n",
    "            latents, args.prompt, optimizer, save_dir, multi_apply_fn\n",
    "        )\n",
    "        best_image.save(f\"{save_dir}/best_image.png\")\n",
    "        init_image.save(f\"{save_dir}/init_image.png\")\n",
    "    elif args.task == \"example-prompts\":\n",
    "        fo = open(\"assets/example_prompts.txt\", \"r\")\n",
    "        prompts = fo.readlines()\n",
    "        fo.close()\n",
    "        for i, prompt in tqdm(enumerate(prompts)):\n",
    "            # Get new latents and optimizer\n",
    "            init_latents = torch.randn(shape, device=device, dtype=dtype)\n",
    "            latents = torch.nn.Parameter(init_latents, requires_grad=enable_grad)\n",
    "            optimizer = get_optimizer(args.optim, latents, args.lr, args.nesterov)\n",
    "\n",
    "            prompt = prompt.strip()\n",
    "            name = f\"{i:03d}_{prompt[:150]}.png\"\n",
    "            save_dir = f\"{args.save_dir}/{args.task}/{settings}/{name}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            init_image, best_image, init_rewards, best_rewards = trainer.train(\n",
    "                latents, prompt, optimizer, save_dir, multi_apply_fn\n",
    "            )\n",
    "            if i == 0:\n",
    "                total_best_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "                total_init_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "            for k in best_rewards.keys():\n",
    "                total_best_rewards[k] += best_rewards[k]\n",
    "                total_init_rewards[k] += init_rewards[k]\n",
    "            best_image.save(f\"{save_dir}/best_image.png\")\n",
    "            init_image.save(f\"{save_dir}/init_image.png\")\n",
    "            logging.info(f\"Initial rewards: {init_rewards}\")\n",
    "            logging.info(f\"Best rewards: {best_rewards}\")\n",
    "        for k in total_best_rewards.keys():\n",
    "            total_best_rewards[k] /= len(prompts)\n",
    "            total_init_rewards[k] /= len(prompts)\n",
    "\n",
    "        # save results to directory\n",
    "        with open(f\"{args.save_dir}/example-prompts/{settings}/results.txt\", \"w\") as f:\n",
    "            f.write(\n",
    "                f\"Mean initial all rewards: {total_init_rewards}\\n\"\n",
    "                f\"Mean best all rewards: {total_best_rewards}\\n\"\n",
    "            )\n",
    "    elif args.task == \"t2i-compbench\":\n",
    "        prompt_list_file = f\"../T2I-CompBench/examples/dataset/{args.prompt}.txt\"\n",
    "        fo = open(prompt_list_file, \"r\")\n",
    "        prompts = fo.readlines()\n",
    "        fo.close()\n",
    "        os.makedirs(f\"{args.save_dir}/{args.task}/{settings}/samples\", exist_ok=True)\n",
    "        for i, prompt in tqdm(enumerate(prompts)):\n",
    "            # Get new latents and optimizer\n",
    "            init_latents = torch.randn(shape, device=device, dtype=dtype)\n",
    "            latents = torch.nn.Parameter(init_latents, requires_grad=enable_grad)\n",
    "            optimizer = get_optimizer(args.optim, latents, args.lr, args.nesterov)\n",
    "\n",
    "            prompt = prompt.strip()\n",
    "            init_image, best_image, init_rewards, best_rewards = trainer.train(\n",
    "                latents, prompt, optimizer, None, multi_apply_fn\n",
    "            )\n",
    "            if i == 0:\n",
    "                total_best_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "                total_init_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "            for k in best_rewards.keys():\n",
    "                total_best_rewards[k] += best_rewards[k]\n",
    "                total_init_rewards[k] += init_rewards[k]\n",
    "            name = f\"{prompt}_{i:06d}.png\"\n",
    "            best_image.save(f\"{args.save_dir}/{args.task}/{settings}/samples/{name}\")\n",
    "            logging.info(f\"Initial rewards: {init_rewards}\")\n",
    "            logging.info(f\"Best rewards: {best_rewards}\")\n",
    "        for k in total_best_rewards.keys():\n",
    "            total_best_rewards[k] /= len(prompts)\n",
    "            total_init_rewards[k] /= len(prompts)\n",
    "    elif args.task == \"parti-prompts\":\n",
    "        parti_dataset = load_dataset(\"nateraw/parti-prompts\", split=\"train\")\n",
    "        total_reward_diff = 0.0\n",
    "        total_best_reward = 0.0\n",
    "        total_init_reward = 0.0\n",
    "        total_improved_samples = 0\n",
    "        for index, sample in enumerate(parti_dataset):\n",
    "            os.makedirs(\n",
    "                f\"{args.save_dir}/{args.task}/{settings}/{index}\", exist_ok=True\n",
    "            )\n",
    "            prompt = sample[\"Prompt\"]\n",
    "            init_image, best_image, init_rewards, best_rewards = trainer.train(\n",
    "                latents, prompt, optimizer, multi_apply_fn\n",
    "            )\n",
    "            best_image.save(\n",
    "                f\"{args.save_dir}/{args.task}/{settings}/{index}/best_image.png\"\n",
    "            )\n",
    "            open(\n",
    "                f\"{args.save_dir}/{args.task}/{settings}/{index}/prompt.txt\", \"w\"\n",
    "            ).write(\n",
    "                f\"{prompt} \\n Initial Rewards: {init_rewards} \\n Best Rewards: {best_rewards}\"\n",
    "            )\n",
    "            logging.info(f\"Initial rewards: {init_rewards}\")\n",
    "            logging.info(f\"Best rewards: {best_rewards}\")\n",
    "            initial_reward = init_rewards[args.benchmark_reward]\n",
    "            best_reward = best_rewards[args.benchmark_reward]\n",
    "            total_reward_diff += best_reward - initial_reward\n",
    "            total_best_reward += best_reward\n",
    "            total_init_reward += initial_reward\n",
    "            if best_reward < initial_reward:\n",
    "                total_improved_samples += 1\n",
    "            if i == 0:\n",
    "                total_best_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "                total_init_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "            for k in best_rewards.keys():\n",
    "                total_best_rewards[k] += best_rewards[k]\n",
    "                total_init_rewards[k] += init_rewards[k]\n",
    "            # Get new latents and optimizer\n",
    "            init_latents = torch.randn(shape, device=device, dtype=dtype)\n",
    "            latents = torch.nn.Parameter(init_latents, requires_grad=enable_grad)\n",
    "            optimizer = get_optimizer(args.optim, latents, args.lr, args.nesterov)\n",
    "        improvement_percentage = total_improved_samples / parti_dataset.num_rows\n",
    "        mean_best_reward = total_best_reward / parti_dataset.num_rows\n",
    "        mean_init_reward = total_init_reward / parti_dataset.num_rows\n",
    "        mean_reward_diff = total_reward_diff / parti_dataset.num_rows\n",
    "        logging.info(\n",
    "            f\"Improvement percentage: {improvement_percentage:.4f}, \"\n",
    "            f\"mean initial reward: {mean_init_reward:.4f}, \"\n",
    "            f\"mean best reward: {mean_best_reward:.4f}, \"\n",
    "            f\"mean reward diff: {mean_reward_diff:.4f}\"\n",
    "        )\n",
    "        for k in total_best_rewards.keys():\n",
    "            total_best_rewards[k] /= len(parti_dataset)\n",
    "            total_init_rewards[k] /= len(parti_dataset)\n",
    "        # save results\n",
    "        os.makedirs(f\"{args.save_dir}/parti-prompts/{settings}\", exist_ok=True)\n",
    "        with open(f\"{args.save_dir}/parti-prompts/{settings}/results.txt\", \"w\") as f:\n",
    "            f.write(\n",
    "                f\"Mean improvement: {improvement_percentage:.4f}, \"\n",
    "                f\"mean initial reward: {mean_init_reward:.4f}, \"\n",
    "                f\"mean best reward: {mean_best_reward:.4f}, \"\n",
    "                f\"mean reward diff: {mean_reward_diff:.4f}\\n\"\n",
    "                f\"Mean initial all rewards: {total_init_rewards}\\n\"\n",
    "                f\"Mean best all rewards: {total_best_rewards}\"\n",
    "            )\n",
    "    elif args.task == \"geneval\":\n",
    "        prompt_list_file = \"../geneval/prompts/evaluation_metadata.jsonl\"\n",
    "        with open(prompt_list_file) as fp:\n",
    "            metadatas = [json.loads(line) for line in fp]\n",
    "        outdir = f\"{args.save_dir}/{args.task}/{settings}\"\n",
    "        for index, metadata in enumerate(metadatas):\n",
    "            # Get new latents and optimizer\n",
    "            init_latents = torch.randn(shape, device=device, dtype=dtype)\n",
    "            latents = torch.nn.Parameter(init_latents, requires_grad=True)\n",
    "            optimizer = get_optimizer(args.optim, latents, args.lr, args.nesterov)\n",
    "\n",
    "            prompt = metadata[\"prompt\"]\n",
    "            init_image, best_image, init_rewards, best_rewards = trainer.train(\n",
    "                latents, prompt, optimizer, None, multi_apply_fn\n",
    "            )\n",
    "            logging.info(f\"Initial rewards: {init_rewards}\")\n",
    "            logging.info(f\"Best rewards: {best_rewards}\")\n",
    "            outpath = f\"{outdir}/{index:0>5}\"\n",
    "            os.makedirs(f\"{outpath}/samples\", exist_ok=True)\n",
    "            with open(f\"{outpath}/metadata.jsonl\", \"w\") as fp:\n",
    "                json.dump(metadata, fp)\n",
    "            best_image.save(f\"{outpath}/samples/{args.seed:05}.png\")\n",
    "            if i == 0:\n",
    "                total_best_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "                total_init_rewards = {k: 0.0 for k in best_rewards.keys()}\n",
    "            for k in best_rewards.keys():\n",
    "                total_best_rewards[k] += best_rewards[k]\n",
    "                total_init_rewards[k] += init_rewards[k]\n",
    "        for k in total_best_rewards.keys():\n",
    "            total_best_rewards[k] /= len(parti_dataset)\n",
    "            total_init_rewards[k] /= len(parti_dataset)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task {args.task}\")\n",
    "    # log total rewards\n",
    "    logging.info(f\"Mean initial rewards: {total_init_rewards}\")\n",
    "    logging.info(f\"Mean best rewards: {total_best_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--cache_dir CACHE_DIR]\n",
      "                             [--save_dir SAVE_DIR] [--model MODEL] [--lr LR]\n",
      "                             [--n_iters N_ITERS]\n",
      "                             [--n_inference_steps N_INFERENCE_STEPS]\n",
      "                             [--optim {sgd,adam,lbfgs}] [--nesterov]\n",
      "                             [--grad_clip GRAD_CLIP] [--seed SEED]\n",
      "                             [--disable_hps] [--hps_weighting HPS_WEIGHTING]\n",
      "                             [--disable_imagereward]\n",
      "                             [--imagereward_weighting IMAGEREWARD_WEIGHTING]\n",
      "                             [--disable_clip]\n",
      "                             [--clip_weighting CLIP_WEIGHTING]\n",
      "                             [--disable_pickscore]\n",
      "                             [--pickscore_weighting PICKSCORE_WEIGHTING]\n",
      "                             [--disable_aesthetic]\n",
      "                             [--aesthetic_weighting AESTHETIC_WEIGHTING]\n",
      "                             [--disable_reg] [--reg_weight REG_WEIGHT]\n",
      "                             [--task {t2i-compbench,single,parti-prompts,geneval,example-prompts}]\n",
      "                             [--prompt PROMPT]\n",
      "                             [--benchmark_reward {ImageReward,PickScore,HPS,CLIP,total}]\n",
      "                             [--save_all_images] [--no_optim] [--imageselect]\n",
      "                             [--memsave] [--dtype DTYPE]\n",
      "                             [--device_id DEVICE_ID] [--cpu_offloading]\n",
      "                             [--enable_multi_apply]\n",
      "                             [--multi_step_model MULTI_STEP_MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/aseth7/.local/share/jupyter/runtime/kernel-v332e11f5a3f5d11656623af40c2ad2a271a8af7c1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
